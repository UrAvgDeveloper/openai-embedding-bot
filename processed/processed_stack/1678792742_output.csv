title,text
Publishing Contracts,Publishing Contracts. # Publishing Contracts This is an overview of how to publish the contract's source code in this repo. We use Cargo's default registry crates.io at https://crates.io/ for publishing contracts written in Rust.
Publishing Contracts,"Preparation. ,Updating schema: To allow easy use of the contract, we can publish the schema () together with the source code. Ensure you check in all the schema files, and make a git commit with the final state. This commit will be published and should be tagged. Generally, you will want to tag with the version (eg. ), but in the repo, we have multiple contracts and label it like . Don't forget a ,Note on build results: Build results like Wasm bytecode or expected hash don't need to be updated since the don't belong to the source publication. However, they are excluded from packaging in which allows you to commit them to your git repository if you like. A single source code can be built with multiple different optimizers, so we should not make any strict assumptions on the tooling that will be used. ## Publishing Now that your package is properly configured and all artifacts are committed, it is time to share it with the world. Please refer to the complete instructions for any questions at https://rurust.github.io/cargo-docs-ru/crates-io.html, but I will try to give a quick overview of the happy path here. ,Registry: You will need an account on crates.io at https://crates.io to publish a rust crate. If you don't have one already, just click on ""Log in with GitHub"" in the top-right to quickly set up a free account. Once inside, click on your username (top-right), then ""Account Settings"". On the bottom, there is a section called ""API Access"". If you don't have this set up already, create a new token and use to set it up. This will now authenticate you with the cli tool and allow you to publish. ,Uploading: Once this is set up, make sure you commit the current state you want to publish. Then try . If that works well, review the files that will be published via . If you are satisfied, you can now officially publish it via . Congratulations, your package is public to the world. ,Sharing: Once you have published your package, people can now find it by searching for ""cw-"" on crates.io at https://crates.io/search?q=cw. But that isn't exactly the simplest way. To make things easier and help keep the ecosystem together, we suggest making a PR to add your package to the at https://github.com/cosmwasm/cawesome-wasm list. ,Organizations: Many times you are writing a contract not as a solo developer, but rather as part of an organization. You will want to allow colleagues to upload new versions of the contract to crates.io when you are on holiday. [These instructions show how]() you can set up your crate to allow multiple maintainers. You can add another owner to the crate by specifying their github user. Note, you will now both have complete control of the crate, and they can remove you: You can also add an existing github team inside your organization: The team will allow anyone who is currently in the team to publish new versions of the crate. And this is automatically updated when you make changes on github. However, it will not allow anyone in the team to add or remove other owners."
Publishing Contracts,"Publishing. ,Registry: You will need an account on crates.io at https://crates.io to publish a rust crate. If you don't have one already, just click on ""Log in with GitHub"" in the top-right to quickly set up a free account. Once inside, click on your username (top-right), then ""Account Settings"". On the bottom, there is a section called ""API Access"". If you don't have this set up already, create a new token and use to set it up. This will now authenticate you with the cli tool and allow you to publish. ,Uploading: Once this is set up, make sure you commit the current state you want to publish. Then try . If that works well, review the files that will be published via . If you are satisfied, you can now officially publish it via . Congratulations, your package is public to the world. ,Sharing: Once you have published your package, people can now find it by searching for ""cw-"" on crates.io at https://crates.io/search?q=cw. But that isn't exactly the simplest way. To make things easier and help keep the ecosystem together, we suggest making a PR to add your package to the at https://github.com/cosmwasm/cawesome-wasm list. ,Organizations: Many times you are writing a contract not as a solo developer, but rather as part of an organization. You will want to allow colleagues to upload new versions of the contract to crates.io when you are on holiday. [These instructions show how]() you can set up your crate to allow multiple maintainers. You can add another owner to the crate by specifying their github user. Note, you will now both have complete control of the crate, and they can remove you: You can also add an existing github team inside your organization: The team will allow anyone who is currently in the team to publish new versions of the crate. And this is automatically updated when you make changes on github. However, it will not allow anyone in the team to add or remove other owners."
Importing,"Importing. # Importing In Publishing at ./Publishing.md, we discussed how you can publish your contract to the world. This looks at the flip-side, how can you use someone else's contract (which is the same question as how they will use your contract). Let's go through the various stages."
Importing,"Verifying Artifacts. Before using remote code, you most certainly want to verify it is honest. The simplest audit of the repo is to simply check that the artifacts in the repo are correct. This involves recompiling the claimed source with the claimed builder and validating that the locally compiled code (hash) matches the code hash that was uploaded. This will verify that the source code is the correct preimage. Which allows one to audit the original (Rust) source code, rather than looking at wasm bytecode. We have a script to do this automatic verification steps that can easily be run by many individuals. Please check out at https://github.com/CosmWasm/cosmwasm-verify/blob/master/README.md to see a simple shell script that does all these steps and easily allows you to verify any uploaded contract."
Importing,"Reviewing. Once you have done the quick programatic checks, it is good to give at least a quick look through the code. A glance at to make sure it is outputing all relevant structs from , and also ensure is just the default wrapper (nothing funny going on there). After this point, we can dive into the contract code itself. Check the flows for the execute methods, any invariants and permission checks that should be there, and a reasonable data storage format. You can dig into the contract as far as you want, but it is important to make sure there are no obvious backdoors at least."
Importing,"Decentralized Verification. It's not very practical to do a deep code review on every dependency you want to use, which is a big reason for the popularity of code audits in the blockchain world. We trust some experts review in lieu of doing the work ourselves. But wouldn't it be nice to do this in a decentralized manner and peer-review each other's contracts? Bringing in deeper domain knowledge and saving fees. Luckily, there is an amazing project called crev at https://github.com/crev-dev/cargo-crev/blob/master/cargo-crev/README.md that provides . I highly recommend that CosmWasm contract developers get set up with this. At minimum, we can all add a review on a package that programmatically checked out that the json schemas and wasm bytecode do match the code, and publish our claim, so we don't all rely on some central server to say it validated this. As we go on, we can add deeper reviews on standard packages. If you want to use , please follow their getting started guide at https://github.com/crev-dev/cargo-crev/blob/master/cargo-crev/src/doc/getting_started.md and once you have made your own *proof repository* with at least one *trust proof*, please make a PR to the []() repo with a link to your repo and some public name or pseudonym that people know you by. This allows people who trust you to also reuse your proofs. There is a standard list of proof repos at https://github.com/crev-dev/cargo-crev/wiki/List-of-Proof-Repositories with some strong rust developers in there. This may cover dependencies like and but will not hit any CosmWasm-related modules, so we look to bootstrap a very focused review community."
Developing,"Developing. # Developing If you have recently created a contract with this template, you probably could use some help on how to build and test the contract, as well as prepare it for production. This file attempts to provide a brief overview, assuming you have installed a recent version of Rust already (eg. 1.51.0+)."
Developing,"Prerequisites. Before starting, make sure you have rustup at https://rustup.rs/ along with a recent and version installed. Currently, we are testing on 1.51.0+. And you need to have the target installed as well. You can check that via:"
Developing,"Compiling and running tests. ,Understanding the tests: The main code is in and the unit tests there run in pure rust, which makes them very quick to execute and give nice output on failures, especially if you do . We consider testing critical for anything on a blockchain, and recommend to always keep the tests up to date."
Developing,"Generating JSON Schema. While the Wasm calls (, , ) accept JSON, this is not enough information to use it. We need to expose the schema for the expected messages to the clients. You can generate this schema by calling , which will output 4 files in , corresponding to the 3 message types the contract accepts, as well as the internal . These files are in standard json-schema format, which should be usable by various client side tools, either to auto-generate codecs, or just to validate incoming json wrt. the defined schema."
Developing,"Preparing the Wasm bytecode for production. Before we upload it to a chain, we need to ensure the smallest output size possible, as this will be included in the body of a transaction. We also want to have a reproducible build process, so third parties can verify that the uploaded Wasm code did indeed come from the claimed rust code. To solve both these issues, we have produced , a docker image to produce an extremely small build output in a consistent manner. The suggest way to run it is this: We must mount the contract code to . You can use a absolute path instead of if you don't want to to the directory first. The other two volumes are nice for speedup. Mounting in particular is useful to avoid docker overwriting your local dev files with root permissions. Note the cache is unique for each contract being compiled to limit interference, while the registry cache is global. This is rather slow compared to local compilations, especially the first compile of a given contract. The use of the two volume caches is very useful to speed up following compiles of the same contract. This produces an directory with a , as well as , containing the Sha256 hash of the wasm file. The wasm file is compiled deterministically (anyone else running the same docker on the same git commit should get the identical file with the same Sha256 hash). It is also stripped and minimized for upload to a blockchain (we will also gzip it in the uploading process to make it even smaller)."
"""Hello Cosmos""","""Hello Cosmos"". # ""Hello Cosmos"" ""Hello World"" but with friends! Projects generated from the template will include a fully functional example DApp frontend, integrating with the current Fetch test-net at https://docs.fetch.ai/ledger_v2/networks/#test-nets directly as well as a cosmwasm contract deployed to the test-net. The ""Hello Cosmos"" example DApp demonstrates the following kinds of interactions: - CosmWasm contract query - CosmWasm contract call - Native token transfer - Fetch / Keplr wallet integration ### Features - Print greetings submitted by other users - Connect with Fetch wallet - Print address - Print balance - Submit greeting - (one greeting per address) - Tip other users - (half testnet balance)"
Create React App Template,"Create React App Template. # Create React App Template Create React App at https://create-react-app.dev is a CLI utility which facilitates generation of react app boilerplate via templates. In order to expedite distributed application (DApp) development, we maintain a create-react-app template: cra-template-cosmjs-keplr at https://github.com/fetchai/cra-template-cosmjs-keplr. Projects generated from this template include all the necessary dependencies and build configuration needed to: - Interact with the Fetch or Keplr wallet - fetchai/fetch-wallet (github) at https://github.com/fetchai/fetch-wallet/blob/master/packages/provider/src/core.ts#L43 - Interact with cosmos-based networks - @cosmjs/stargate (npm) at https://www.npmjs.com/package/@cosmjs/stargate - @cosmjs/cosmwasm-stargate (npm) at https://www.npmjs.com/package/@cosmjs/cosmwasm-stargate --- To generate a new project: The generated project also comes with an example DApp at ./example_dapp, ""Hello Cosmos"", which demonstrates Cosmjs and Keplr API usage in the form of a simple frontend."
Welcome to the Fetch.ai Collective Learning Library,"Welcome to the Fetch.ai Collective Learning Library. # Welcome to the Fetch.ai Collective Learning Library Colearn is a library that enables privacy-preserving decentralized machine learning tasks on the FET network. This blockchain-mediated collective learning system enables multiple stakeholders to build a shared machine learning model without needing to rely on a central authority, and without revealing their dataset to the other stakeholders. This library is currently in development."
Welcome to the Fetch.ai Collective Learning Library,"How collective learning works. ,Current Version: We have released *v.0.2.8* of the Colearn Machine Learning Interface, the first version of an interface that allows developers to define their own model architectures that can then be used in collective learning. Together with the interface we provide a simple backend for local experiments. This is a prototype backend with upcoming blockchain ledger based backends to follow. Future releases will use similar interfaces so that learners built with the current system will work on a different backend that integrates a distributed ledger and provides other improvements. The current framework will then be used mainly for model development and debugging. We invite all users to experiment with the framework, develop their own models, and provide feedback!"
Welcome to the Fetch.ai Collective Learning Library,Getting Started. 
Welcome to the Fetch.ai Collective Learning Library,"Writing your own models. We encourage users to try out the system by writing their own models. Models need to implement the collective learning interface, which provides functions for training and voting on updates. More instructions can be found in the Getting Started section."
Using collective learning with keras,"Using collective learning with keras. # Using collective learning with keras This tutorial is a simple guide to trying out the collective learning protocol with your own machine learning code. Everything runs locally. The most flexible way to use the collective learning backends is to make a class that implements the Collective Learning defined in ml_interface.py at {{ repo_root }}/colearn/ml_interface.py. For more details on how to use the see here at ./intro_tutorial_mli.md However, the simpler way is to use one of the helper classes that we have provided that implement most of the interface for popular ML libraries. In this tutorial we are going to walk through using the . First we are going to define the model architecture, then we are going to load the data and configure the model, and then we will run Collective Learning. A standard script for machine learning with Keras looks like the one below There are three steps: 1. Load the data 2. Define the model 3. Train the model In this tutorial we are going to see how to modify each step to use collective learning. We'll end up with code like this: The first thing is to modify the data loading code. Each learner needs to have their own training and testing set from the data. This is easy to do with keras: The model definition is very similar too, except that each learner will need its own copy of the model, so we've moved it into a function. To use collective learning, we need to create an object that implements the MachineLearningInterface. To make it easier to use the with keras, we've defined . implements standard training and evaluation routines as well as the MachineLearningInterface methods. We create a set of KerasLearners by passing in the model and the datasets: Then we give all the models the same weights to start off with: And then we can move on to the final stage, which is training with Collective Learning. The function performs one round of collective learning. One learner is selected to train and propose an update. The other learners vote on the update, and if the vote passes then the update is accepted. Then a new round begins."
gRPC tutorial,"gRPC tutorial. # gRPC tutorial This tutorial explains how to set up the gRPC learner server. It assumes that you can already run colearn locally, and that you have already defined your own models and dataloaders (if you're going to do so). If you haven't done this then see the tutorials in the Getting Started at ./intro_tutorial_mli.md section."
gRPC tutorial,"Architecture of colearn. There are two main parts to a collective learning system: the learner and the backend. The backend controls the learner, and manages the smart contracts and IPFS, and acts as a control hub for all the associated learners. The learner is the part that executes machine learning code. This consists of proposing, evaluating and accepting new weights as detailed in the Machine Learning Interface. The learner and the backend communicate via gRPC at https://grpc.io; the learner runs a gRPC server, and the backend runs a gRPC client that makes requests of the learner. This separation means that the learner can run on specialised hardware (e.g. a compute server) and does not need to be co-located with the backend."
gRPC tutorial,"Architecture of gRPC server. The gRPC interface is defined in colearn_grpc/proto/interface.proto at {{ repo_root }}/colearn_grpc/proto/interface.proto. This defines the functions that the gRPC server exposes and the format for messages between the server and the client. As we covered in the earlier tutorials, the machine learning part of colearn is contained inside the (MLI). To recap: the MLI provides methods for proposing, evaluating and accepting weights. If you want to use your own models with colearn then you need to write an object that implements the MLI (for example, an instance of a python class that inherits from ). For more about the MLI see the MLI tutorial at ./intro_tutorial_mli.md. The gRPC server has an MLI factory, and it uses its MLI factory to make objects that implement the . The MLI factory needs to implement the MLI factory interface. You could write your own MLI factory, but it's easier to use the one we provide. Below we will discuss the MLI factory interface and then talk about how to use the example factory."
gRPC tutorial,MLI Factory interface. 
gRPC tutorial,"Using the example MLI Factory. The example MLI factory is defined in colearn_grpc/example_mli_factory.py at {{ repo_root }}/colearn_grpc/example_mli_factory.py. It stores the models and dataloaders that it knows about in factoryRegistry.py To add a new model and dataloader to the factory you need to do the following things: 1. Define a function that loads the dataset given the location of the dataset. 2. Define a function that takes in the dataset and loads the MLI model. 3. Register both these functions with the factory registry. Registering a dataloader looks like this: Registering a model is similar, but you additionally have to specify the dataloaders that this model is compatible with. You can see an example of how to do this in colearn_examples/grpc/mnist_grpc.py at {{ repo_root }}/colearn_examples/grpc/mnist_grpc.py. The FactoryRegistry decorators get evaluated when the functions are imported, so ensure that the functions are imported before constructing the gRPC server (more on that later). Constraints on the dataloader function: 1. The first parameter should be a mandatory parameter called ""location"" which stores the location of the dataset. 2. The subsequent parameters should have default arguments. 3. The return type should be specified with a type annotation, and this should be the same type that is expected by the model functions that use this dataloader. 4. The arguments that you pass to the dataloader function must be JSON-encodable at https://docs.python.org/3.7/library/json.html. Native python types are fine (e.g. str, dict, list, float). Constraints on the model function: 1. The first parameter should be a mandatory parameter called ""data_loaders"". This must have the same type as the return type of the compatible dataloaders. 2. The subsequent parameters should have default arguments. 3. The return type of model_function should be or a subclass of it (e.g. ). 4. The dataloaders listed as being compatible with the model should already be registered with FactoryRegistry before the model is registered. 4. The arguments that you pass to the model function must be JSON-encodable at https://docs.python.org/3.7/library/json.html. Native python types are fine (e.g. str, dict, list, float)."
gRPC tutorial,"Making it all work together. It can be challenging to ensure that all the parts talk to each other, so we have provided some examples and helper scripts. It is recommended to first make an all-in-one script following the example of colearn_examples/grpc/mnist_grpc.py at {{ repo_root }}/colearn_examples/grpc/mnist_grpc.py. Once this is working you can run colearn_grpc/scripts/run_n_servers.py at {{ repo_root }}/colearn_grpc/scripts/run_n_servers.py or colearn_grpc/scripts/run_grpc_server.py at {{ repo_root }}/colearn_grpc/scripts/run_server.py to run the server(s). The script colearn_grpc/scripts/probe_grpc_server.py at {{ repo_root }}/colearn_grpc/scripts/probe_grpc_server.py will connect to a gRPC server and print the dataloaders and models that are registered on it (pass in the address as a parameter). The client side of the gRPC communication can then be run using colearn_examples/grpc/run_grpc_demo.py at {{ repo_root }}/colearn_examples/grpc/run_grpc_demo.py. More details are given below. A note about running tensorflow in multiple processes: on a system with a GPU, tensorflow will try to get all the GPU memory when it starts up. This means that running tensorflow in multiple processes on the same machine will fail. To prevent this happening, tensorflow should be told to use only the CPU by setting the environment variable to . This can be done in a python script (before importing tensorflow) by using:"
gRPC tutorial,"Testing locally with an all-in-one script. You can test this locally by following the example in colearn_examples/grpc/mnist_grpc.py at {{ repo_root }}/colearn_examples/grpc/mnist_grpc.py. Define your dataloader and model functions as specified above, and register them with the factory. Then create n_learners gRPC servers: And then create n_learners gRPC clients: inherits from the so you can use it with the training functions as before:"
gRPC tutorial,"Testing remotely. We expect that the gRPC learner part will often be on a compute cluster and be separate from the gRPC client side. To test the gRPC in a setup like this you can start the servers on the computer side and the client part separately. For one gRPC server: For multiple gRPC servers: The servers by default will start on port 9995 and use subsequent ports from there, so if three servers are required they will run on ports 9995, 9996 and 9997. If you have written your own dataloaders and models then you need to make sure that those functions are defined or imported before the server is created. These are the imports of the default dataloaders and models in : Once the gRPC server(s) are running, set up whatever networking and port forwarding is required. You can check that the gRPC server is accessible by using the probe script: If the connection is successful this will print a list of the models and datasets registered on the server. These are the defaults that are registered: Then run on the other side to run the usual demo. The script takes as arguments the model name and dataset name that should be run, along with the number of learners and the data location for each learner."
gRPC tutorial,Using the MLI Factory interface. An alternative method of using your own dataloaders and models with the gRPC server is to use the MLI Factory interface. This is defined in . An example is given in . The MLI Factory is implemented as shown: An instance of the class needs to be passed to the gRPC server on creation: The rest of the example follows the example.
Developer Notes,Developer Notes. # Developer Notes These are some notes for developers working on the colearn code repo
Developer Notes,Google Cloud Storage. To have access to the google cloud storage you need to set up your google authentication and have the $GOOGLE_APPLICATION_CREDENTIALS set up correctly. For more details ask or see the contract-learn documentation
Developer Notes,Build image. To build ML server image and push to google cloud use the following command:
Using collective learning with pytorch,"Using collective learning with pytorch. # Using collective learning with pytorch This tutorial is a simple guide to trying out the collective learning protocol with your own machine learning code. Everything runs locally. The most flexible way to use the collective learning backends is to make a class that implements the Collective Learning defined in ml_interface.py at {{ repo_root }}/colearn/ml_interface.py. For more details on how to use the see here at ./intro_tutorial_mli.md However, the simpler way is to use one of the helper classes that we have provided that implement most of the interface for popular ML libraries. In this tutorial we are going to walk through using the . First we are going to define the model architecture, then we are going to load the data and configure the model, and then we will run Collective Learning. A standard script for machine learning with Pytorch looks like the one below There are three steps: 1. Load the data 2. Define the model 3. Train the model In this tutorial we are going to see how to modify each step to use collective learning. We'll end up with code like this: The first thing is to modify the data loading code. Each learner needs to have their own training and testing set from the data. This is easy to do with the pytorch random_split utility: The model definition is the same as before. To use collective learning, we need to create an object that implements the MachineLearningInterface. To make it easier to use the with pytorch, we've defined . implements standard training and evaluation routines as well as the MachineLearningInterface methods. We create a set of PytorchLearners by passing in the model and the datasets: Then we give all the models the same weights to start off with: And then we can move on to the final stage, which is training with Collective Learning. The function performs one round of collective learning. One learner is selected to train and propose an update. The other learners vote on the update, and if the vote passes then the update is accepted. Then a new round begins."
